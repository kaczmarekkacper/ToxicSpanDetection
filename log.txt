Iteration nr 1, train_score: 0.5322536330019839, test_score: 0.6275329389209808, losses : 31044.146810152946
Iteration nr 2, train_score: 0.574633419688047, test_score: 0.6368913636652096, losses : 27329.306323611156
Iteration nr 3, train_score: 0.5470300983856817, test_score: 0.6358498231270701, losses : 25374.764527319647
Iteration nr 4, train_score: 0.6003250325716237, test_score: 0.6578328041294652, losses : 25048.271090570324
Iteration nr 5, train_score: 0.5906518506271949, test_score: 0.6442079213128052, losses : 24099.324736239127
Iteration nr 6, train_score: 0.6066901590356057, test_score: 0.6504674331282114, losses : 23986.33071765292
Iteration nr 7, train_score: 0.6214217762460172, test_score: 0.6546691682945603, losses : 22932.602674049558
Iteration nr 8, train_score: 0.6040087510871919, test_score: 0.6576049769420083, losses : 22374.59387362939
Iteration nr 9, train_score: 0.6145146896018986, test_score: 0.6538444996884941, losses : 21312.336915577343
Iteration nr 10, train_score: 0.590438133780776, test_score: 0.644595618873944, losses : 21901.09166140864
Iteration nr 11, train_score: 0.6071498269560097, test_score: 0.6498523742651795, losses : 20463.948105714604
Iteration nr 12, train_score: 0.5952974377989612, test_score: 0.6446898584906393, losses : 21749.12541445237
Iteration nr 13, train_score: 0.5879667599188718, test_score: 0.6407878972300461, losses : 20204.22892974973
Iteration nr 14, train_score: 0.6314212346616733, test_score: 0.6543060442882622, losses : 20286.675740656894
Iteration nr 15, train_score: 0.6331999930629455, test_score: 0.6545898747455953, losses : 18664.47791070029
Iteration nr 16, train_score: 0.6445783456613267, test_score: 0.6590279298548131, losses : 19411.359005136408
Iteration nr 17, train_score: 0.6441950739922103, test_score: 0.6499508319579834, losses : 19868.25659642769
Iteration nr 18, train_score: 0.6348594126984317, test_score: 0.6470089119213525, losses : 19595.914615809772
Iteration nr 19, train_score: 0.6127627118242607, test_score: 0.6524454043336788, losses : 18833.08654332493
Iteration nr 20, train_score: 0.6458322921347023, test_score: 0.6601433216320999, losses : 18643.190997534122
Iteration nr 21, train_score: 0.6288897766477719, test_score: 0.6493875280182138, losses : 18731.551513768954
Iteration nr 22, train_score: 0.6562722207556758, test_score: 0.6597097283723569, losses : 18636.78935039318
Iteration nr 23, train_score: 0.6317849791889296, test_score: 0.6563141129408652, losses : 18244.82764435272
Iteration nr 24, train_score: 0.6300807660511272, test_score: 0.6512631904403404, losses : 18272.28308835659
Iteration nr 25, train_score: 0.6380795625213402, test_score: 0.6538098628767128, losses : 17271.4067664406
Iteration nr 26, train_score: 0.64879838409462, test_score: 0.6476864939419049, losses : 17725.45807145938
Iteration nr 27, train_score: 0.6474525846688399, test_score: 0.6555479743735161, losses : 17637.505037166673
Iteration nr 28, train_score: 0.6556170268230874, test_score: 0.6585937534510127, losses : 18795.68182220216
Iteration nr 29, train_score: 0.6434187191432725, test_score: 0.6587612919797415, losses : 17862.5485196936
Iteration nr 30, train_score: 0.6294994912607529, test_score: 0.6486365166053955, losses : 17410.704275748856
Iteration nr 31, train_score: 0.6579013173780403, test_score: 0.6552750201977736, losses : 17815.59739488126
Iteration nr 32, train_score: 0.65375221075558, test_score: 0.6571851576028529, losses : 17520.26237211564
Iteration nr 33, train_score: 0.653789474912576, test_score: 0.649657341475546, losses : 17414.542242300042
Iteration nr 34, train_score: 0.6474584709910282, test_score: 0.6504475958713379, losses : 17522.155903877087
Iteration nr 35, train_score: 0.6609544201483856, test_score: 0.6557459220001627, losses : 17149.233524112977
Iteration nr 36, train_score: 0.6704260906618354, test_score: 0.6533751041016435, losses : 17070.782649140023
Iteration nr 37, train_score: 0.6700155315492722, test_score: 0.6507218851464296, losses : 16660.369832577435
Iteration nr 38, train_score: 0.6422436106425419, test_score: 0.6392539497764512, losses : 16621.424788895296
Iteration nr 39, train_score: 0.6508491742627946, test_score: 0.6420407343981975, losses : 16695.246992249995
Iteration nr 40, train_score: 0.6591168417653699, test_score: 0.6449117068388068, losses : 16313.06308791013
Iteration nr 41, train_score: 0.6692562061892582, test_score: 0.6514798721389693, losses : 16557.569114459908
Iteration nr 42, train_score: 0.668807213643384, test_score: 0.6505957061270357, losses : 15959.547372206132
Iteration nr 43, train_score: 0.6802941185830176, test_score: 0.6536171121580404, losses : 16424.319714311394
Iteration nr 44, train_score: 0.68124181756282, test_score: 0.6457897298020936, losses : 15846.462708248695
Iteration nr 45, train_score: 0.6840561174969121, test_score: 0.654528128804901, losses : 16258.375859691496
Iteration nr 46, train_score: 0.6685968532318408, test_score: 0.6500284992470533, losses : 16731.856885751815
Iteration nr 47, train_score: 0.671280101445253, test_score: 0.6473397788204757, losses : 17113.038905821813
Iteration nr 48, train_score: 0.68431060582111, test_score: 0.658626356327134, losses : 16302.633670253981
Iteration nr 49, train_score: 0.6631382109228106, test_score: 0.6512936241623872, losses : 15847.968213054275
Iteration nr 50, train_score: 0.6801155072741065, test_score: 0.6483305006384696, losses : 16378.129249607415
Iteration nr 51, train_score: 0.6897673335466115, test_score: 0.6499512957763133, losses : 15689.596838695601
Iteration nr 52, train_score: 0.701495393090111, test_score: 0.653263585199893, losses : 15914.49662277729
Iteration nr 53, train_score: 0.6930859011583264, test_score: 0.6437964711382177, losses : 15579.593999769168
Iteration nr 54, train_score: 0.7114567221722327, test_score: 0.6445740627031121, losses : 16013.575050397312
Iteration nr 55, train_score: 0.690863136735631, test_score: 0.648582986500526, losses : 16064.802454179182
Iteration nr 56, train_score: 0.7105241413224055, test_score: 0.6435176286054783, losses : 15730.758041969113
Iteration nr 57, train_score: 0.707987347626903, test_score: 0.6565371463592277, losses : 16009.392423377836
Iteration nr 58, train_score: 0.7178126139070704, test_score: 0.6495698064360073, losses : 15970.112961152568
Iteration nr 59, train_score: 0.7131909690020475, test_score: 0.6486623667718866, losses : 16044.903687673934
Iteration nr 60, train_score: 0.7066700994710028, test_score: 0.6481099828243754, losses : 15928.041933174994
Iteration nr 61, train_score: 0.7072219552867548, test_score: 0.6562007515084262, losses : 15147.642377464043
Iteration nr 62, train_score: 0.711439391396465, test_score: 0.6499044145991086, losses : 15538.446110285999
Iteration nr 63, train_score: 0.7171902386832745, test_score: 0.6480763356578103, losses : 15323.723545668212
Iteration nr 64, train_score: 0.7041047604211053, test_score: 0.650435133715497, losses : 15848.973610995623
Iteration nr 65, train_score: 0.7061720993168785, test_score: 0.647748016580989, losses : 15340.717663106667
Iteration nr 66, train_score: 0.6852830141547976, test_score: 0.649438724686784, losses : 15143.030891224114
Iteration nr 67, train_score: 0.7075487930012887, test_score: 0.6486140527334392, losses : 15662.536941183935
Iteration nr 68, train_score: 0.7293649384490382, test_score: 0.6577024294319546, losses : 15101.20512305591
Iteration nr 69, train_score: 0.7256470937911629, test_score: 0.6582882651083912, losses : 15359.166411945536
Iteration nr 70, train_score: 0.7147186478147254, test_score: 0.6530710708836933, losses : 15534.209145541789
Iteration nr 71, train_score: 0.7243437081766173, test_score: 0.6512095270793496, losses : 15529.256124148133
Iteration nr 72, train_score: 0.7046794928912984, test_score: 0.65325798906185, losses : 15204.87378921612
Iteration nr 73, train_score: 0.7306423800273906, test_score: 0.6504959573741004, losses : 15312.228775058464
Iteration nr 74, train_score: 0.7177009059377693, test_score: 0.6543303023209266, losses : 15115.503430826537
Iteration nr 75, train_score: 0.7207188340238291, test_score: 0.6457193374883349, losses : 14941.995696210095
Iteration nr 76, train_score: 0.7083869861165437, test_score: 0.6460827399455278, losses : 15207.707302668074
Iteration nr 77, train_score: 0.706437362908998, test_score: 0.6484536044094337, losses : 15014.117986442558
Iteration nr 78, train_score: 0.7066903966388844, test_score: 0.6361359031372322, losses : 15424.7908539641
Iteration nr 79, train_score: 0.7206220576347199, test_score: 0.646200554693061, losses : 15002.401457416203
Iteration nr 80, train_score: 0.7337246688874791, test_score: 0.6473272310474724, losses : 14395.158159028982
Iteration nr 81, train_score: 0.7463183111852849, test_score: 0.6489644229307066, losses : 14873.707292961575
Iteration nr 82, train_score: 0.7313425016720329, test_score: 0.6478875134625155, losses : 15126.545854806784
Iteration nr 83, train_score: 0.7387137104004855, test_score: 0.6410189731206362, losses : 14562.218821499795
Iteration nr 84, train_score: 0.7225959050265414, test_score: 0.6483600325198589, losses : 14662.284029134422
Iteration nr 85, train_score: 0.7459253432968255, test_score: 0.6472891422914878, losses : 14411.643876816726
Iteration nr 86, train_score: 0.7372396620622771, test_score: 0.6470326523946044, losses : 14720.581758395194
Iteration nr 87, train_score: 0.7499064839413883, test_score: 0.6448062592696754, losses : 14044.687192387963
Iteration nr 88, train_score: 0.7312590871032604, test_score: 0.6416569610601822, losses : 14508.673033722256
Iteration nr 89, train_score: 0.7346417206331644, test_score: 0.6431322090146854, losses : 14719.616192959633
Iteration nr 90, train_score: 0.7108513957173855, test_score: 0.6428107586964387, losses : 14399.004211412765
Iteration nr 91, train_score: 0.7341922602687834, test_score: 0.6347031552384104, losses : 13976.90528473472
Iteration nr 92, train_score: 0.7715714365671311, test_score: 0.6392425279470796, losses : 14366.24235598055
Iteration nr 93, train_score: 0.7487249626004919, test_score: 0.6578559047782363, losses : 14219.472490023772
Iteration nr 94, train_score: 0.7500752091252004, test_score: 0.64797004697856, losses : 13717.496028266003
Iteration nr 95, train_score: 0.7451499470612843, test_score: 0.649662578398649, losses : 14195.515049275327
Iteration nr 96, train_score: 0.7264987092751157, test_score: 0.6368881151954517, losses : 14726.70540474504
Iteration nr 97, train_score: 0.7290617822354051, test_score: 0.6454507171382966, losses : 14086.313845165767
Iteration nr 98, train_score: 0.7512139235372877, test_score: 0.6403135509160767, losses : 14241.433688503566
Iteration nr 99, train_score: 0.737149662748963, test_score: 0.6410757826418138, losses : 14693.686576652384
Iteration nr 100, train_score: 0.7586141944477909, test_score: 0.6470836578091186, losses : 14077.571415585207
Iteration nr 101, train_score: 0.7653441130534386, test_score: 0.6449321978856024, losses : 14431.584995227457
Iteration nr 102, train_score: 0.7605184260685987, test_score: 0.6444101487084711, losses : 14281.220357126582
Iteration nr 103, train_score: 0.7481444761634, test_score: 0.6460195562377148, losses : 13980.940083684623
Iteration nr 104, train_score: 0.7409155303458863, test_score: 0.6495115221799325, losses : 13884.684606637686
Iteration nr 105, train_score: 0.7533293830067173, test_score: 0.6495756563096555, losses : 14764.56764828647
Iteration nr 106, train_score: 0.732418839722474, test_score: 0.6395518447810713, losses : 13819.957493119713
Iteration nr 107, train_score: 0.7513941963521069, test_score: 0.6372510381174847, losses : 14075.472166499758
Iteration nr 108, train_score: 0.761303342409979, test_score: 0.6561963486896696, losses : 13622.342089422753
Iteration nr 109, train_score: 0.7540001698955618, test_score: 0.6443071688588105, losses : 14158.457859786835
Iteration nr 110, train_score: 0.7520980170009129, test_score: 0.6503656447553267, losses : 13982.909525788169
Iteration nr 111, train_score: 0.7518976202921299, test_score: 0.648531897305354, losses : 13589.423031724627
Iteration nr 112, train_score: 0.7765596954359514, test_score: 0.6455965761521077, losses : 13970.40986726453
Iteration nr 113, train_score: 0.7750247549284074, test_score: 0.645414880587871, losses : 14048.327285466756
Iteration nr 114, train_score: 0.7583526572993742, test_score: 0.6448825573608775, losses : 13419.093251588907
Iteration nr 115, train_score: 0.745918249823251, test_score: 0.6410598919401839, losses : 13795.342575233808
Iteration nr 116, train_score: 0.7593841923341481, test_score: 0.632562503991281, losses : 13860.076382247624
Iteration nr 117, train_score: 0.7531090498406183, test_score: 0.6448415464057077, losses : 13689.185786626274
Iteration nr 118, train_score: 0.7723747473028072, test_score: 0.6534982468246882, losses : 14584.709558874903
Iteration nr 119, train_score: 0.7928950294816033, test_score: 0.6422112798641492, losses : 13767.374734849243
Iteration nr 120, train_score: 0.7550655898224256, test_score: 0.6426533673198102, losses : 14033.15238856208
Iteration nr 121, train_score: 0.794323313554584, test_score: 0.639309705914276, losses : 13394.10397764825
Iteration nr 122, train_score: 0.7773995249977071, test_score: 0.639640597650606, losses : 14079.939953364803
Iteration nr 123, train_score: 0.772394815078886, test_score: 0.6469995316645326, losses : 13942.02427831458
Iteration nr 124, train_score: 0.7876422878037072, test_score: 0.6423954964822951, losses : 13733.49992093506
Iteration nr 125, train_score: 0.7769266211622009, test_score: 0.6487039312683098, losses : 14089.093565081454
Iteration nr 126, train_score: 0.7673535853743297, test_score: 0.6423531197395627, losses : 13819.058163043155
Iteration nr 127, train_score: 0.7809959658544101, test_score: 0.6427811891495873, losses : 13877.92627815629
Iteration nr 128, train_score: 0.7873314346831768, test_score: 0.6376310060031651, losses : 13358.658803603485
Iteration nr 129, train_score: 0.7667895929148923, test_score: 0.6410305377675122, losses : 12621.858987183745
Iteration nr 130, train_score: 0.790619058105161, test_score: 0.6394915211042227, losses : 13326.33620387967
Iteration nr 131, train_score: 0.7686947655744972, test_score: 0.6398989798905141, losses : 13608.168066538266
Iteration nr 132, train_score: 0.7716900398531983, test_score: 0.646879612061643, losses : 13582.18734687883
Iteration nr 133, train_score: 0.7942566448425215, test_score: 0.6426143700648362, losses : 13372.831462049686
Iteration nr 134, train_score: 0.7983060776077437, test_score: 0.6460229940185184, losses : 13552.05988716398
Iteration nr 135, train_score: 0.7753339019937977, test_score: 0.6373055527921927, losses : 13134.730454219964
Iteration nr 136, train_score: 0.8005158760761331, test_score: 0.6362783030844658, losses : 13217.50182717575
Iteration nr 137, train_score: 0.795611146646871, test_score: 0.6372232831362358, losses : 13100.24869373722
Iteration nr 138, train_score: 0.7859358588132618, test_score: 0.6426298448226521, losses : 13490.792500317812
Iteration nr 139, train_score: 0.7652042270500281, test_score: 0.6429863608274053, losses : 13437.054743805536
Iteration nr 140, train_score: 0.7862016419442194, test_score: 0.6459271751866049, losses : 13125.617418350583
Iteration nr 141, train_score: 0.794357870832643, test_score: 0.6385549871903594, losses : 13015.580335288512
Iteration nr 142, train_score: 0.7775002348498833, test_score: 0.6405427268485158, losses : 12927.378970897786
Iteration nr 143, train_score: 0.810430615000504, test_score: 0.6419932282119073, losses : 13396.592602194278
Iteration nr 144, train_score: 0.7975861604470087, test_score: 0.6440570860346561, losses : 13083.185586129908
Iteration nr 145, train_score: 0.7817359642453708, test_score: 0.6431040516272656, losses : 13036.52151635764
Iteration nr 146, train_score: 0.8110988119835738, test_score: 0.6436918001295311, losses : 12630.693500451122
Iteration nr 147, train_score: 0.7874883384394058, test_score: 0.6432211050920936, losses : 13257.112734348646
Iteration nr 148, train_score: 0.8101361312073545, test_score: 0.6436715584685265, losses : 12967.100582173452
Iteration nr 149, train_score: 0.8034493302779175, test_score: 0.6495227418439801, losses : 13103.608844547394
Iteration nr 150, train_score: 0.7760631749684035, test_score: 0.6358374793982308, losses : 12980.710422288044
Iteration nr 151, train_score: 0.7989428846892557, test_score: 0.6404951717043261, losses : 12842.94210464439
Iteration nr 152, train_score: 0.799468852082878, test_score: 0.6387864110899765, losses : 12723.75833762263
Iteration nr 153, train_score: 0.7863998269399111, test_score: 0.6435053033082269, losses : 12714.928613181904
Iteration nr 154, train_score: 0.7892170686400594, test_score: 0.6445673437638989, losses : 13356.742115487457
Iteration nr 155, train_score: 0.7832941110878173, test_score: 0.6332436932033708, losses : 12904.845584417959
Iteration nr 156, train_score: 0.8030492825102213, test_score: 0.6422770772511819, losses : 12739.850402204482
Iteration nr 157, train_score: 0.8150355403654324, test_score: 0.6392426209639713, losses : 12874.392533793816
Iteration nr 158, train_score: 0.7930943277064603, test_score: 0.6385036260880946, losses : 12467.050066005666
Iteration nr 159, train_score: 0.7910384872967844, test_score: 0.6409153835930688, losses : 12895.69121693939
Iteration nr 160, train_score: 0.8009686304772922, test_score: 0.6406245784881576, losses : 12580.765516845257
Iteration nr 161, train_score: 0.805665077827652, test_score: 0.6409089961401878, losses : 12895.575624159104
Iteration nr 162, train_score: 0.7869160218974668, test_score: 0.6386792575059785, losses : 12778.146425470513
Iteration nr 163, train_score: 0.8051000483622729, test_score: 0.6421002472194234, losses : 12422.254360420742
Iteration nr 164, train_score: 0.8171389990237541, test_score: 0.6409586805550466, losses : 12684.125744839434
Iteration nr 165, train_score: 0.810805587691275, test_score: 0.6457368283218601, losses : 12727.684787813541
Iteration nr 166, train_score: 0.8088343045305346, test_score: 0.635589569588992, losses : 12235.053885610907
Iteration nr 167, train_score: 0.8106660473408599, test_score: 0.6355848657012033, losses : 12780.689656738672
Iteration nr 168, train_score: 0.8066467666469258, test_score: 0.6422115599470973, losses : 12749.058109838763
Iteration nr 169, train_score: 0.8181452632782871, test_score: 0.6366502098350079, losses : 12471.651004751633
Iteration nr 170, train_score: 0.8084786702911568, test_score: 0.6376008753954258, losses : 12559.017469836976
Iteration nr 171, train_score: 0.8278281893034405, test_score: 0.6372261427132657, losses : 12352.020462196133
Iteration nr 172, train_score: 0.8124183678631118, test_score: 0.6381165043420262, losses : 12556.387347599695
Iteration nr 173, train_score: 0.8259917687585018, test_score: 0.6382169950693838, losses : 12688.529070944087
Iteration nr 174, train_score: 0.8117240279929627, test_score: 0.6420448021325507, losses : 12173.4398689829
Iteration nr 175, train_score: 0.80486664592199, test_score: 0.6336381483117836, losses : 12064.120064539442
Iteration nr 176, train_score: 0.8081551883574938, test_score: 0.6350169770750916, losses : 12236.258774922442
Iteration nr 177, train_score: 0.8200549392462642, test_score: 0.6332093035205582, losses : 12548.374952560178
Iteration nr 178, train_score: 0.825971715686558, test_score: 0.6341653575715451, losses : 12762.515622329438
Iteration nr 179, train_score: 0.8023480589269701, test_score: 0.6282796679580827, losses : 13001.302651496635
Iteration nr 180, train_score: 0.8141236806049804, test_score: 0.6387906830425429, losses : 11966.515288468729
Iteration nr 181, train_score: 0.8228374758757906, test_score: 0.6356796549717277, losses : 12523.895556181866
Iteration nr 182, train_score: 0.8163345795355647, test_score: 0.6386143195311352, losses : 12361.691473468309
Iteration nr 183, train_score: 0.8244212399242421, test_score: 0.6382166681744025, losses : 12447.958067250252
Iteration nr 184, train_score: 0.8228375734347259, test_score: 0.6343925743964052, losses : 12063.308332028193
Iteration nr 185, train_score: 0.8150725879893564, test_score: 0.640516852113832, losses : 12284.553392712081
Iteration nr 186, train_score: 0.8299789129978491, test_score: 0.6351210973299761, losses : 11895.814981528785
Iteration nr 187, train_score: 0.8456339762367411, test_score: 0.6253481754621222, losses : 12703.376437303377
Iteration nr 188, train_score: 0.8294798629220442, test_score: 0.6384556252501055, losses : 11917.425527383886
Iteration nr 189, train_score: 0.8309536068975446, test_score: 0.6331809249908122, losses : 12165.56333623154
Iteration nr 190, train_score: 0.8236880810094634, test_score: 0.6427866059569635, losses : 11943.451751571642
Iteration nr 191, train_score: 0.8414464347093419, test_score: 0.640176689908738, losses : 12250.849693205226
Iteration nr 192, train_score: 0.8244617208744867, test_score: 0.6443492967288205, losses : 12431.043981113027
Iteration nr 193, train_score: 0.8419375616194442, test_score: 0.6320608274309087, losses : 12091.904534432857
Iteration nr 194, train_score: 0.8257448919046476, test_score: 0.6432391949121571, losses : 11673.45642657855
Iteration nr 195, train_score: 0.8215160568926336, test_score: 0.6316092756709768, losses : 11942.456517360644
Iteration nr 196, train_score: 0.8295873848266528, test_score: 0.6370578143157417, losses : 12005.627343565324
Iteration nr 197, train_score: 0.8228792963826852, test_score: 0.6371288529111435, losses : 12203.637104498326
Iteration nr 198, train_score: 0.8157642200069846, test_score: 0.6358187745626039, losses : 11927.481882470234
Iteration nr 199, train_score: 0.827041531826935, test_score: 0.6374073528229419, losses : 11830.119137321304
Iteration nr 200, train_score: 0.8397956686401339, test_score: 0.6366480209747022, losses : 11999.933649321247